{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# High-Throughput Molecular Screening Demo\n",
    "\n",
    "This notebook demonstrates how to perform large-scale batch inference using the `deepmirror` public API.\n",
    "We'll screen 15 million molecules using a pre-trained model, submitting the job in batches (max 5M per chunk).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure that:\n",
    "1. You are logged in via terminal using `dm login <YOUR_EMAIL>`\n",
    "2. Your API token is saved to file\n",
    "3. Your input data is in **parquet** format with two columns:\n",
    "   - `\"ID\"` (unique identifier)\n",
    "   - `\"SMILES\"` (molecular structure)\n",
    "\n",
    "You can install the `deepmirror` client library below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deepmirror\n",
    "# !dm login <YOUREMAIL>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import io\n",
    "import tempfile\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from deepmirror.api import (\n",
    "    create_batch_inference,\n",
    "    download_batch_results,\n",
    "    get_batch_inference,\n",
    "    list_models,\n",
    ")\n",
    "\n",
    "MAX_ROWS_PER_BATCH = 5_000_000\n",
    "TOTAL_ROWS = 15_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "In this example we will artificially create a 15M row dataset by repeating a smaller dataset multiple times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path().cwd().parent\n",
    "csv_path = root / \"data\" / \"data-reg.csv\"\n",
    "df = pd.read_csv(csv_path)[[\"SMILES\"]]\n",
    "\n",
    "repeat_factor = int(TOTAL_ROWS / len(df))\n",
    "df = pd.concat([df] * repeat_factor, ignore_index=True)\n",
    "df[\"ID\"] = df.index\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Step 2: Save screening library to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.NamedTemporaryFile(\n",
    "    suffix=\".parquet\", delete=False\n",
    ") as tmp_parquet:\n",
    "    df.to_parquet(tmp_parquet.name)\n",
    "    screening_file = tmp_parquet.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Step 3: Select your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list_models()\n",
    "model_id = models[0][\"model_id\"]  # Replace with your desired model ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step 4: Submit batch jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "screening_df = pd.read_parquet(screening_file)\n",
    "assert \"ID\" in screening_df.columns\n",
    "assert \"SMILES\" in screening_df.columns\n",
    "\n",
    "jobs = []\n",
    "for i in range(0, len(screening_df), MAX_ROWS_PER_BATCH):\n",
    "    chunk = screening_df.iloc[i : i + MAX_ROWS_PER_BATCH]\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(\n",
    "        suffix=\".parquet\", delete=False\n",
    "    ) as tmp_chunk:\n",
    "        chunk.to_parquet(tmp_chunk.name)\n",
    "        job = create_batch_inference(\n",
    "            model_id=model_id, file_path=tmp_chunk.name\n",
    "        )\n",
    "        jobs.append(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df = pd.DataFrame(jobs)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "jobs_df.to_csv(f\"batch-inference-{timestamp}.csv\", index=False)\n",
    "jobs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 5: Monitor job progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df = jobs_df.copy()\n",
    "done = set()\n",
    "bar = tqdm(\n",
    "    total=len(jobs_df), desc=\"Batch inference jobs\", position=0, leave=True\n",
    ")\n",
    "\n",
    "while not jobs_df[\"status\"].isin([\"completed\", \"failed\"]).all():\n",
    "    status_summary = []\n",
    "\n",
    "    for idx, row in jobs_df.iterrows():\n",
    "        if row[\"status\"] in (\"completed\", \"failed\"):\n",
    "            status_summary.append(\n",
    "                f\"Job {row['task_id'][:6]}...: {row['status']} ({row['progress']}%)\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        status = get_batch_inference(row[\"task_id\"])\n",
    "        jobs_df.at[idx, \"status\"] = status[\"status\"]\n",
    "        jobs_df.at[idx, \"progress\"] = status[\"progress\"]\n",
    "\n",
    "        line = f\"Job {row['task_id'][:6]}...: {status['status']} ({status['progress']}%)\"\n",
    "        status_summary.append(line)\n",
    "\n",
    "        if status[\"status\"] in (\"completed\", \"failed\") and idx not in done:\n",
    "            bar.update(1)\n",
    "            done.add(idx)\n",
    "\n",
    "    bar.set_postfix_str(\"\\n\".join(status_summary))\n",
    "    time.sleep(5)\n",
    "\n",
    "bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 6: Review job completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "completed = jobs_df[jobs_df[\"status\"] == \"completed\"]\n",
    "failed = jobs_df[jobs_df[\"status\"] == \"failed\"]\n",
    "\n",
    "print(f\"Completed jobs: {len(completed)}\")\n",
    "print(f\"Failed jobs: {len(failed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Step 7: Download predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, job in completed.iterrows():\n",
    "    result_bytes = download_batch_results(job[\"task_id\"])\n",
    "    result_df = pd.read_parquet(io.BytesIO(result_bytes))\n",
    "    result_df.to_csv(f\"example-output-{job['task_id']}.csv\", index=False)\n",
    "\n",
    "result_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
